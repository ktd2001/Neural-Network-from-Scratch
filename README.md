# Neural-Network-from-Scratch

Context:
To demonstrate my knowledge of Neural Networks, I created a Neural Network that trained and tested data without the help of built in Python packages. 

Project:
The Neural Network used dummy data with 5 input features and 2 output features. Feed forward and back propogation algorithms were utilized. The sigmoid and relu activation functions were both tested with gradient descent employed to optimize the SSR value. 

I randomly initialized the weights and biases for each layer, Using matrix multiplication to work through the feed forward process. For back propagation, I utilized Gradient Descent for optimization with SSR as my evaluation metric. Seeking to shrink SSR, gradient descent optimizes the parameters, in this case, the weights and biases until maximum number of epochs have been reached or maximum accuracy. For the activation function, I provided both sigmoid and relu as either alternative. For the data the network is run on, I preferred utilizing sigmoid over relu as it provided more accurate predictions. You can find my complete code in the alternative tab. This was a fun and interesting project that solidified my understanding in how Neural Networks work.
